\documentclass{proposal}

\usepackage{times}  
\usepackage{hyperref}
\usepackage{titlesec}

\hypersetup{pdfstartview=FitH,pdfpagelayout=SinglePage}

\setlength\paperheight {11in}
\setlength\paperwidth {8.5in}
\setlength{\textwidth}{7in}
\setlength{\textheight}{9.25in}
\setlength{\oddsidemargin}{-.25in}
\setlength{\evensidemargin}{-.25in}

\begin{document}

% \conferenceinfo{HotNets 2022} {}
% \CopyrightYear{2022}
% \crdata{X}
% \date{}

\title{CS395T Project Proposal: Using Fine-grained Queue Metrics to Scale Microservices
}

\author{Rahul Menon, Leo Orshansky, Esther Yoon}

\maketitle

%%%%%%%%%%%%%  ABSTRACT GOES HERE %%%%%%%%%%%%%%
% \begin{abstract}
% \end{abstract}

\section{Project Idea}
% Describe the project and how it relates to the course
% Prior work and how it falls short, novelty of our approach
There has been a trend towards building latency-sensitive applications with many single-purpose microservices as opposed to with monolithic services. Connected microservices are structured in the form of a DAG, where each microservice represents a node, and the containers for these services are typically provisioned over a collection of servers.

Resource management becomes a key challenge when working with distributed microservices. Meeting service level objectives (SLO) without overprovisioning resources is critical for providing performance guarantees and maintaining minimal operating costs for service providers. However, it can be difficult to identify where performance issues or bottlenecks originate from within complex microservice graphs.

Typically there are two forms of scaling used for the resource management of microservices. \textit{Vertical scaling} is a fine-grained scaling technique that modifies resource limits, such as CPU usage and I/O bandwidth usage. \textit{Horizontal scaling} is a more coarse-grained approach that adjusts the number of replicas of a microservice. Oftentimes, a global controller is used to perform either type of scaling.

There are many prior approaches that leverage horizontal or vertical scaling to address microservice resource management. Sinan is a vertical scaling approach that uses an ML model to predict the end-to-end performance of per-tier resource allocations and scales resource limits accordingly. Another vertical scaler is AutoThrottle, which uses a combination of an application-level controller that uses online RL to set performance targets and per-microservice controllers to perform fine-grained CPU scaling based on the targets set. FIRM is a horizontal scaler which identifies bottleneck services using reinforcement learning. 

These prior solutions use global metrics like end-to-end latency or local resource utilization metrics to scale microservices. However, two metrics for microservice performance that aren't often leveraged, yet can be much more informative, are the request arrival rates and request processing (depature) rates of each microservice. These rates can also be thought of as the ingress and egress \textit{queue lengths} of a microservice. If queue lengths are known at any given time of an application's execution, the real-time latencies of individual microservices could be clearly understood. With this latency knowledge, these \textit{queue length-based} solutions could more precisely identify bottlenecks in microservices that can subsequently be scaled up by a global controller. Powerchief is one such queue length-based method, however, it requires modifying an application for it to support request rate collection.

% Main solution
To our knowledge, there is no queue length-based solution for autoscaling microservices that collects request rates in a way that is transparent to the application. For our project, we plan to build a service mesh to transparently capture queueing statistics through the sidecars attached to each microservice. Sidecars can capture request arrival and departure rates by acting as proxies for their respective microservices. Since all application traffic is passed through the proxy, we anticipate our final implementation to be able to successfully and easily collect statistics for any application using our service mesh framework. Sidecars will then transfer rate data to a global controller that performs analysis and horizontal scaling. For the global controller, we will formulate a heuristic-based policy using queue lengths and potentially other collected metrics. For more detailed goals, see Sections \ref{implementation} and \ref{outcomes}.

% Challenges
Our approach may require significant instrumentation to support transparent request rate collection, including setting up the service mesh and figuring out the best way to collect queue length statistics. In addition to this challenge, we will have to understand how to manage the communication overhead between sidecars and the centralized controller. For example, higher frequencies of communication will lead to more fine-grained information for the controller to act on, but will incur higher communication overheads.

Other challenges to this approach include accounting for replicated microservice startup time, evaluating how local scaling policies impact end-to-end performance, and examining how request arrival and departure rates may be impacted by dependencies among microservices.

\section{Implementation Plan} \label{implementation}
% What work must be done, and how it will be divided amonst us
% 75%, 100% and 125% goals
We have decided a logical division of this project's goals into a temporal timeline, as well as a subdivision of certain tasks into parallelizable parts to more effectively utilize our group of three. Here is said timeline:
\begin{enumerate}
    \item Initial steps and framework creation
    \begin{itemize}
        \item Setup of container services: mostly will involve writing scripts to handle horizontal scaling yaml configs and kubernetes (Esther)
        \item Sidecar/service mesh setup: general setup/scripting and implementation of request/response rate collection through the sidecar (Leo)
        \item Global controller setup: implement global knob tuning through the controller (scaling of the \# of service instances and resource limits) (Rahul)
    \end{itemize}
\end{enumerate}
\section{Resources}
% Resources we'll be using
\subsection{CloudLab}
We will be utilizing the class's CloudLab project to allocate a testing cluster for our project. This resource will become necessary for our project's success for a few reasons:
\begin{itemize}
    \item A collaborative testing environment which each of us can $\tt ssh$ into creates a standardized testing environment for each group member to work in, and removes the overhead of setting up and debugging tests on the disparate environments of our personal machines.
    \item The CloudLab cluster comes with an abundance of compute and memory resources, which will come in handy to be able to simulate a workload of realistic size on our test microservice mesh.
    \item In order to truly benchmark our auto-scaler solution, we will need more a single-machine simulation of a sample workload on the modified microservice mesh. This is due to the limited ability of a personal machine (with, say, 8 cores) to accurately model the effect of a networked and distributed microservice architecture. We might encounter false positive and false negatives in tuning the global scaling controller if we do so on a single-machine setup instead of the more accurate model of a multi-machine cloud.
\end{itemize}
Using CloudLab we will create a virtual cloud with enough machines to model many different possible microservice mesh layouts (assignments of microservices to physical/virtual machines), and this way be able to produce accurate benchmarks in the process of developing our scaler.

\section{Outcomes}\label{outcomes}
% Questions to be answered
% Final evaluation plan and experiments to run
\subsection{Experiments}
In evaluating our method, we want to understand some performance characteristics of the containerized microservice system. Particularly, we want to understand how our implementation compares to existing methods for scaling microservices under some fixed style of workload typical of microservices. Some of the methods we may be interested in comparing against are the Kubernetes Horizontal Pod Autoscaling \cite{kubernetes_hpa}, pHPA \cite{choi_phpa}, and COLA \cite{sachi_cola}. Interesting metrics would include end-to-end latency from the client's perspective and tracking hardware usage over time (CPU, memory, IO, etc.). 

We may also want to investigate how different containerization topologies affect the performance of our system. That is, how does the placement of different services in different arrangements across available compute nodes change the behavior of our system. In this situation, we would again keep the workload fixed and measure both end-to-end latency and hardware resource usage.

We also predict that there is some tradeoff between the overhead of telemetry being delivered to the controller and the controller's responsiveness. Understanding this tradeoff would also involve trying out different kinds of controllers with different expected frequencies of updates from nodes. Experimental setup would be similar to the above. 

\subsection{Evaluation}
\paragraph*{75\% Goal} For a basic goal, we would want to have an experimental setup running which allows the sidecar proxies to send telemetry to a ``dumb'' controller which does not act on the information. This is a good lower bound on the work for this project as it would allow for at least a minimum understanding of how telemetry gathering in the service mesh affects the performance characteristics.

\paragraph*{100\% Goal} For our expected goal, we would expect to implement an autoscaler that listens to telemetry data from the service mesh and improves overall performance, in addition to the requirements above.

\paragraph*{125\% Goal} For a stretch goal, we would be able to explore more complex autoscalers and telemetry gathering mechanisms. For example, machine learning based autoscalers would fit into this goal, as well as understanding characteristics of different kinds of microservices to scale them differently.

\subsection{Impact}
% Impact if successful 
If our project is successful, we imagine potential positive impacts for microservice deployments. In particular, we could see more efficient and dynamic cloud resource usage for microservice applications, which may result in lower power consumption and lower resource costs. In addition, we may see more responsive services on the client-side, which would be a general quality-of-life improvment for users of microservice-based applications.

\bibliographystyle{abbrv} 
\begin{small}
\bibliography{proposal}
\end{small}

\end{document}

